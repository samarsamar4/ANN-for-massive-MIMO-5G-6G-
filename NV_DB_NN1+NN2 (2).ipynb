{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import decimal\n",
    "from numpy.linalg import inv\n",
    "from commpy.modulation import QAMModem\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Dropout, Conv1D,MaxPooling1D,GlobalAveragePooling1D, Flatten\n",
    "from decimal import getcontext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "getcontext().Emax = 600000000000\n",
    "import import_ipynb\n",
    "from Utils import *\n",
    "IBO = 3\n",
    "p = 1.1\n",
    "q = 4\n",
    "Vsat = 1.9\n",
    "G = 16\n",
    "A = -345\n",
    "B = 0.17\n",
    "Mr = 10\n",
    "Mt = 100\n",
    "M = 16\n",
    "N_bits = Mr * np.log2(M)\n",
    "SNRdb = decimal.Decimal(60000000000)\n",
    "Niter = 50\n",
    "mu1 = 0.0017\n",
    "lamda = 0.00071\n",
    "mu2 = 1\n",
    "eps = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmsede GD -39.89514241380351\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0459 - mse: 0.0034 - val_loss: 0.0417 - val_mse: 0.0028\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0393 - mse: 0.0025 - val_loss: 0.0371 - val_mse: 0.0022\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0355 - mse: 0.0020 - val_loss: 0.0339 - val_mse: 0.0018\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0327 - mse: 0.0017 - val_loss: 0.0312 - val_mse: 0.0016\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0301 - mse: 0.0015 - val_loss: 0.0286 - val_mse: 0.0013\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0276 - mse: 0.0012 - val_loss: 0.0262 - val_mse: 0.0011\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0253 - mse: 0.0011 - val_loss: 0.0240 - val_mse: 9.4873e-04\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0230 - mse: 8.8538e-04 - val_loss: 0.0217 - val_mse: 7.9329e-04\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0208 - mse: 7.3666e-04 - val_loss: 0.0195 - val_mse: 6.5416e-04\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 6.0367e-04 - val_loss: 0.0174 - val_mse: 5.3022e-04\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0165 - mse: 4.8673e-04 - val_loss: 0.0153 - val_mse: 4.2158e-04\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0145 - mse: 3.8535e-04 - val_loss: 0.0133 - val_mse: 3.2819e-04\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0125 - mse: 2.9857e-04 - val_loss: 0.0113 - val_mse: 2.4867e-04\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0106 - mse: 2.2524e-04 - val_loss: 0.0094 - val_mse: 1.8303e-04\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 1.6696e-04 - val_loss: 0.0077 - val_mse: 1.3385e-04\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 1.2464e-04 - val_loss: 0.0064 - val_mse: 9.9733e-05\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 9.4112e-05 - val_loss: 0.0052 - val_mse: 7.3021e-05\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 6.7384e-05 - val_loss: 0.0040 - val_mse: 4.9041e-05\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0036 - mse: 4.3411e-05 - val_loss: 0.0029 - val_mse: 2.9641e-05\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0025 - mse: 2.5377e-05 - val_loss: 0.0020 - val_mse: 1.6515e-05\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0017 - mse: 1.3452e-05 - val_loss: 0.0014 - val_mse: 8.1735e-06\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0012 - mse: 6.3859e-06 - val_loss: 9.8058e-04 - val_mse: 3.6520e-06\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 8.8972e-04 - mse: 2.8178e-06 - val_loss: 7.8559e-04 - val_mse: 1.7126e-06\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 7.4168e-04 - mse: 1.3802e-06 - val_loss: 6.8478e-04 - val_mse: 9.5475e-07\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.6587e-04 - mse: 8.3945e-07 - val_loss: 6.3697e-04 - val_mse: 7.0276e-07\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.3084e-04 - mse: 6.8293e-07 - val_loss: 6.2302e-04 - val_mse: 6.6076e-07\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.2308e-04 - mse: 6.6281e-07 - val_loss: 6.1893e-04 - val_mse: 6.5365e-07\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1776e-04 - mse: 6.5139e-07 - val_loss: 6.1785e-04 - val_mse: 6.4763e-07\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1436e-04 - mse: 6.4446e-07 - val_loss: 6.1482e-04 - val_mse: 6.4542e-07\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1406e-04 - mse: 6.4450e-07 - val_loss: 6.1106e-04 - val_mse: 6.3564e-07\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1362e-04 - mse: 6.4329e-07 - val_loss: 6.1212e-04 - val_mse: 6.3706e-07\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1256e-04 - mse: 6.4036e-07 - val_loss: 6.1354e-04 - val_mse: 6.4126e-07\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1407e-04 - mse: 6.4370e-07 - val_loss: 6.1399e-04 - val_mse: 6.4328e-07\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1407e-04 - mse: 6.4355e-07 - val_loss: 6.1310e-04 - val_mse: 6.3962e-07\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1403e-04 - mse: 6.4397e-07 - val_loss: 6.1413e-04 - val_mse: 6.4411e-07\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1410e-04 - mse: 6.4410e-07 - val_loss: 6.1282e-04 - val_mse: 6.4063e-07\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1418e-04 - mse: 6.4385e-07 - val_loss: 6.1348e-04 - val_mse: 6.4035e-07\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1280e-04 - mse: 6.4263e-07 - val_loss: 6.1270e-04 - val_mse: 6.3806e-07\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1197e-04 - mse: 6.3970e-07 - val_loss: 6.1296e-04 - val_mse: 6.3568e-07\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1266e-04 - mse: 6.4157e-07 - val_loss: 6.1236e-04 - val_mse: 6.3401e-07\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1267e-04 - mse: 6.4111e-07 - val_loss: 6.1367e-04 - val_mse: 6.4006e-07\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1245e-04 - mse: 6.4085e-07 - val_loss: 6.1486e-04 - val_mse: 6.4407e-07\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1436e-04 - mse: 6.4324e-07 - val_loss: 6.1389e-04 - val_mse: 6.4194e-07\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1256e-04 - mse: 6.4245e-07 - val_loss: 6.1390e-04 - val_mse: 6.4013e-07\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1308e-04 - mse: 6.4150e-07 - val_loss: 6.1357e-04 - val_mse: 6.4549e-07\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1290e-04 - mse: 6.4142e-07 - val_loss: 6.1228e-04 - val_mse: 6.3912e-07\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1225e-04 - mse: 6.4137e-07 - val_loss: 6.1058e-04 - val_mse: 6.3565e-07\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1356e-04 - mse: 6.4230e-07 - val_loss: 6.1233e-04 - val_mse: 6.3868e-07\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1257e-04 - mse: 6.4146e-07 - val_loss: 6.1519e-04 - val_mse: 6.4530e-07\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1283e-04 - mse: 6.4208e-07 - val_loss: 6.1354e-04 - val_mse: 6.4091e-07\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1349e-04 - mse: 6.4344e-07 - val_loss: 6.1412e-04 - val_mse: 6.4223e-07\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1225e-04 - mse: 6.4031e-07 - val_loss: 6.1580e-04 - val_mse: 6.4740e-07\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1293e-04 - mse: 6.4151e-07 - val_loss: 6.1389e-04 - val_mse: 6.3998e-07\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1315e-04 - mse: 6.4163e-07 - val_loss: 6.1294e-04 - val_mse: 6.4220e-07\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1252e-04 - mse: 6.4185e-07 - val_loss: 6.1446e-04 - val_mse: 6.4413e-07\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1268e-04 - mse: 6.4079e-07 - val_loss: 6.1193e-04 - val_mse: 6.3914e-07\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1321e-04 - mse: 6.4192e-07 - val_loss: 6.1228e-04 - val_mse: 6.3774e-07\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1374e-04 - mse: 6.4347e-07 - val_loss: 6.1592e-04 - val_mse: 6.4362e-07\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1367e-04 - mse: 6.4450e-07 - val_loss: 6.1574e-04 - val_mse: 6.4078e-07\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1473e-04 - mse: 6.4453e-07 - val_loss: 6.1275e-04 - val_mse: 6.4180e-07\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1445e-04 - mse: 6.4462e-07 - val_loss: 6.1837e-04 - val_mse: 6.4926e-07\n",
      "Epoch 62/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1491e-04 - mse: 6.4565e-07 - val_loss: 6.1862e-04 - val_mse: 6.5149e-07\n",
      "Epoch 63/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1487e-04 - mse: 6.4525e-07 - val_loss: 6.1538e-04 - val_mse: 6.4306e-07\n",
      "Epoch 64/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1299e-04 - mse: 6.4212e-07 - val_loss: 6.1310e-04 - val_mse: 6.3808e-07\n",
      "Epoch 65/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1287e-04 - mse: 6.4148e-07 - val_loss: 6.1004e-04 - val_mse: 6.3402e-07\n",
      "Epoch 66/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1129e-04 - mse: 6.3818e-07 - val_loss: 6.1256e-04 - val_mse: 6.4057e-07\n",
      "Epoch 67/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1268e-04 - mse: 6.4143e-07 - val_loss: 6.1181e-04 - val_mse: 6.3533e-07\n",
      "Epoch 68/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1248e-04 - mse: 6.4045e-07 - val_loss: 6.1300e-04 - val_mse: 6.4070e-07\n",
      "Epoch 69/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1274e-04 - mse: 6.4201e-07 - val_loss: 6.1521e-04 - val_mse: 6.4135e-07\n",
      "Epoch 70/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1312e-04 - mse: 6.4247e-07 - val_loss: 6.1213e-04 - val_mse: 6.3758e-07\n",
      "Epoch 71/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1341e-04 - mse: 6.4394e-07 - val_loss: 6.1342e-04 - val_mse: 6.3993e-07\n",
      "Epoch 72/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1360e-04 - mse: 6.4193e-07 - val_loss: 6.1338e-04 - val_mse: 6.4223e-07\n",
      "Epoch 73/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1294e-04 - mse: 6.4142e-07 - val_loss: 6.1331e-04 - val_mse: 6.4117e-07\n",
      "Epoch 74/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1284e-04 - mse: 6.4248e-07 - val_loss: 6.1649e-04 - val_mse: 6.4499e-07\n",
      "Epoch 75/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1372e-04 - mse: 6.4245e-07 - val_loss: 6.1163e-04 - val_mse: 6.3839e-07\n",
      "Epoch 76/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1378e-04 - mse: 6.4343e-07 - val_loss: 6.1355e-04 - val_mse: 6.4270e-07\n",
      "Epoch 77/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1398e-04 - mse: 6.4460e-07 - val_loss: 6.1676e-04 - val_mse: 6.4241e-07\n",
      "Epoch 78/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1468e-04 - mse: 6.4537e-07 - val_loss: 6.1773e-04 - val_mse: 6.4622e-07\n",
      "Epoch 79/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1443e-04 - mse: 6.4379e-07 - val_loss: 6.1523e-04 - val_mse: 6.4708e-07\n",
      "Epoch 80/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1454e-04 - mse: 6.4462e-07 - val_loss: 6.1331e-04 - val_mse: 6.4401e-07\n",
      "Epoch 81/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1239e-04 - mse: 6.4089e-07 - val_loss: 6.1153e-04 - val_mse: 6.3523e-07\n",
      "Epoch 82/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1319e-04 - mse: 6.4202e-07 - val_loss: 6.0967e-04 - val_mse: 6.3093e-07\n",
      "Epoch 83/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1328e-04 - mse: 6.4234e-07 - val_loss: 6.1435e-04 - val_mse: 6.4188e-07\n",
      "Epoch 84/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1274e-04 - mse: 6.4137e-07 - val_loss: 6.1637e-04 - val_mse: 6.4293e-07\n",
      "Epoch 85/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1433e-04 - mse: 6.4439e-07 - val_loss: 6.1540e-04 - val_mse: 6.4507e-07\n",
      "Epoch 86/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1396e-04 - mse: 6.4417e-07 - val_loss: 6.1439e-04 - val_mse: 6.4295e-07\n",
      "Epoch 87/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1439e-04 - mse: 6.4410e-07 - val_loss: 6.1326e-04 - val_mse: 6.4170e-07\n",
      "Epoch 88/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1315e-04 - mse: 6.4184e-07 - val_loss: 6.1168e-04 - val_mse: 6.4030e-07\n",
      "Epoch 89/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1265e-04 - mse: 6.4156e-07 - val_loss: 6.1029e-04 - val_mse: 6.3430e-07\n",
      "Epoch 90/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1314e-04 - mse: 6.4283e-07 - val_loss: 6.1315e-04 - val_mse: 6.3680e-07\n",
      "Epoch 91/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1421e-04 - mse: 6.4379e-07 - val_loss: 6.1500e-04 - val_mse: 6.4380e-07\n",
      "Epoch 92/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1376e-04 - mse: 6.4320e-07 - val_loss: 6.1134e-04 - val_mse: 6.3690e-07\n",
      "Epoch 93/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1344e-04 - mse: 6.4309e-07 - val_loss: 6.1543e-04 - val_mse: 6.4107e-07\n",
      "Epoch 94/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1355e-04 - mse: 6.4349e-07 - val_loss: 6.1310e-04 - val_mse: 6.3979e-07\n",
      "Epoch 95/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1362e-04 - mse: 6.4293e-07 - val_loss: 6.1194e-04 - val_mse: 6.4078e-07\n",
      "Epoch 96/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1297e-04 - mse: 6.4220e-07 - val_loss: 6.1165e-04 - val_mse: 6.3589e-07\n",
      "Epoch 97/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1333e-04 - mse: 6.4179e-07 - val_loss: 6.1677e-04 - val_mse: 6.4714e-07\n",
      "Epoch 98/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1500e-04 - mse: 6.4530e-07 - val_loss: 6.1569e-04 - val_mse: 6.4850e-07\n",
      "Epoch 99/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1429e-04 - mse: 6.4438e-07 - val_loss: 6.1623e-04 - val_mse: 6.5147e-07\n",
      "Epoch 100/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1378e-04 - mse: 6.4363e-07 - val_loss: 6.1483e-04 - val_mse: 6.4752e-07\n",
      "Epoch 101/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1381e-04 - mse: 6.4290e-07 - val_loss: 6.1617e-04 - val_mse: 6.4985e-07\n",
      "Epoch 102/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1457e-04 - mse: 6.4563e-07 - val_loss: 6.1285e-04 - val_mse: 6.3959e-07\n",
      "Epoch 103/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1345e-04 - mse: 6.4328e-07 - val_loss: 6.1293e-04 - val_mse: 6.3775e-07\n",
      "Epoch 104/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1288e-04 - mse: 6.4149e-07 - val_loss: 6.1569e-04 - val_mse: 6.4612e-07\n",
      "Epoch 105/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1400e-04 - mse: 6.4359e-07 - val_loss: 6.1660e-04 - val_mse: 6.4675e-07\n",
      "Epoch 106/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1435e-04 - mse: 6.4541e-07 - val_loss: 6.1666e-04 - val_mse: 6.4650e-07\n",
      "Epoch 107/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1470e-04 - mse: 6.4460e-07 - val_loss: 6.1450e-04 - val_mse: 6.4102e-07\n",
      "Epoch 108/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1492e-04 - mse: 6.4616e-07 - val_loss: 6.1110e-04 - val_mse: 6.3477e-07\n",
      "Epoch 109/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1379e-04 - mse: 6.4412e-07 - val_loss: 6.1281e-04 - val_mse: 6.3881e-07\n",
      "Epoch 110/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1585e-04 - mse: 6.4730e-07 - val_loss: 6.1561e-04 - val_mse: 6.4349e-07\n",
      "Epoch 111/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1361e-04 - mse: 6.4239e-07 - val_loss: 6.1304e-04 - val_mse: 6.3726e-07\n",
      "Epoch 112/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1240e-04 - mse: 6.4178e-07 - val_loss: 6.1262e-04 - val_mse: 6.3454e-07\n",
      "Epoch 113/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1320e-04 - mse: 6.4267e-07 - val_loss: 6.1450e-04 - val_mse: 6.4314e-07\n",
      "Epoch 114/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1451e-04 - mse: 6.4453e-07 - val_loss: 6.1482e-04 - val_mse: 6.4198e-07\n",
      "Epoch 115/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1376e-04 - mse: 6.4237e-07 - val_loss: 6.1685e-04 - val_mse: 6.4705e-07\n",
      "Epoch 116/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1452e-04 - mse: 6.4506e-07 - val_loss: 6.1329e-04 - val_mse: 6.4020e-07\n",
      "Epoch 117/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1423e-04 - mse: 6.4434e-07 - val_loss: 6.1509e-04 - val_mse: 6.4093e-07\n",
      "Epoch 118/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1323e-04 - mse: 6.4261e-07 - val_loss: 6.1062e-04 - val_mse: 6.3249e-07\n",
      "Epoch 119/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1289e-04 - mse: 6.4159e-07 - val_loss: 6.1318e-04 - val_mse: 6.4025e-07\n",
      "Epoch 120/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1297e-04 - mse: 6.4201e-07 - val_loss: 6.1511e-04 - val_mse: 6.3992e-07\n",
      "Epoch 121/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1346e-04 - mse: 6.4218e-07 - val_loss: 6.1439e-04 - val_mse: 6.4037e-07\n",
      "Epoch 122/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1355e-04 - mse: 6.4383e-07 - val_loss: 6.1374e-04 - val_mse: 6.3765e-07\n",
      "Epoch 123/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1379e-04 - mse: 6.4392e-07 - val_loss: 6.1529e-04 - val_mse: 6.4229e-07\n",
      "Epoch 124/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1414e-04 - mse: 6.4398e-07 - val_loss: 6.1369e-04 - val_mse: 6.3797e-07\n",
      "Epoch 125/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1476e-04 - mse: 6.4427e-07 - val_loss: 6.1595e-04 - val_mse: 6.4799e-07\n",
      "Epoch 126/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1504e-04 - mse: 6.4612e-07 - val_loss: 6.1629e-04 - val_mse: 6.4452e-07\n",
      "Epoch 127/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1504e-04 - mse: 6.4522e-07 - val_loss: 6.1728e-04 - val_mse: 6.4465e-07\n",
      "Epoch 128/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1444e-04 - mse: 6.4425e-07 - val_loss: 6.1703e-04 - val_mse: 6.4675e-07\n",
      "Epoch 129/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1518e-04 - mse: 6.4616e-07 - val_loss: 6.1598e-04 - val_mse: 6.4646e-07\n",
      "Epoch 130/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1386e-04 - mse: 6.4400e-07 - val_loss: 6.1118e-04 - val_mse: 6.3823e-07\n",
      "Epoch 131/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1286e-04 - mse: 6.4164e-07 - val_loss: 6.1431e-04 - val_mse: 6.4172e-07\n",
      "Epoch 132/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1445e-04 - mse: 6.4500e-07 - val_loss: 6.1509e-04 - val_mse: 6.4213e-07\n",
      "Epoch 133/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1422e-04 - mse: 6.4422e-07 - val_loss: 6.1566e-04 - val_mse: 6.4700e-07\n",
      "Epoch 134/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1501e-04 - mse: 6.4526e-07 - val_loss: 6.1504e-04 - val_mse: 6.4781e-07\n",
      "Epoch 135/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1523e-04 - mse: 6.4645e-07 - val_loss: 6.1327e-04 - val_mse: 6.3826e-07\n",
      "Epoch 136/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1332e-04 - mse: 6.4320e-07 - val_loss: 6.1249e-04 - val_mse: 6.3944e-07\n",
      "Epoch 137/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1349e-04 - mse: 6.4181e-07 - val_loss: 6.1206e-04 - val_mse: 6.4124e-07\n",
      "Epoch 138/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1341e-04 - mse: 6.4262e-07 - val_loss: 6.1385e-04 - val_mse: 6.3960e-07\n",
      "Epoch 139/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1390e-04 - mse: 6.4380e-07 - val_loss: 6.1500e-04 - val_mse: 6.4075e-07\n",
      "Epoch 140/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1469e-04 - mse: 6.4475e-07 - val_loss: 6.1393e-04 - val_mse: 6.3802e-07\n",
      "Epoch 141/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1213e-04 - mse: 6.3959e-07 - val_loss: 6.1049e-04 - val_mse: 6.3479e-07\n",
      "Epoch 142/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1186e-04 - mse: 6.4084e-07 - val_loss: 6.1482e-04 - val_mse: 6.4047e-07\n",
      "Epoch 143/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1328e-04 - mse: 6.4202e-07 - val_loss: 6.1508e-04 - val_mse: 6.4512e-07\n",
      "Epoch 144/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1274e-04 - mse: 6.4139e-07 - val_loss: 6.1260e-04 - val_mse: 6.3942e-07\n",
      "Epoch 145/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1169e-04 - mse: 6.3950e-07 - val_loss: 6.1224e-04 - val_mse: 6.3717e-07\n",
      "Epoch 146/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1090e-04 - mse: 6.3804e-07 - val_loss: 6.1300e-04 - val_mse: 6.4016e-07\n",
      "Epoch 147/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1277e-04 - mse: 6.4151e-07 - val_loss: 6.1225e-04 - val_mse: 6.3480e-07\n",
      "Epoch 148/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1291e-04 - mse: 6.4227e-07 - val_loss: 6.1452e-04 - val_mse: 6.4228e-07\n",
      "Epoch 149/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1340e-04 - mse: 6.4198e-07 - val_loss: 6.1462e-04 - val_mse: 6.4349e-07\n",
      "Epoch 150/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1420e-04 - mse: 6.4436e-07 - val_loss: 6.1469e-04 - val_mse: 6.4069e-07\n",
      "Epoch 151/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1430e-04 - mse: 6.4389e-07 - val_loss: 6.1711e-04 - val_mse: 6.4614e-07\n",
      "Epoch 152/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1455e-04 - mse: 6.4470e-07 - val_loss: 6.1662e-04 - val_mse: 6.4745e-07\n",
      "Epoch 153/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1495e-04 - mse: 6.4569e-07 - val_loss: 6.1627e-04 - val_mse: 6.4456e-07\n",
      "Epoch 154/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1412e-04 - mse: 6.4360e-07 - val_loss: 6.1308e-04 - val_mse: 6.4276e-07\n",
      "Epoch 155/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1361e-04 - mse: 6.4342e-07 - val_loss: 6.1145e-04 - val_mse: 6.3725e-07\n",
      "Epoch 156/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1321e-04 - mse: 6.4268e-07 - val_loss: 6.1160e-04 - val_mse: 6.3648e-07\n",
      "Epoch 157/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1276e-04 - mse: 6.4065e-07 - val_loss: 6.1369e-04 - val_mse: 6.4297e-07\n",
      "Epoch 158/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1387e-04 - mse: 6.4384e-07 - val_loss: 6.1577e-04 - val_mse: 6.4540e-07\n",
      "Epoch 159/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1588e-04 - mse: 6.4762e-07 - val_loss: 6.1862e-04 - val_mse: 6.4700e-07\n",
      "Epoch 160/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1651e-04 - mse: 6.4846e-07 - val_loss: 6.1451e-04 - val_mse: 6.3889e-07\n",
      "Epoch 161/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1580e-04 - mse: 6.4815e-07 - val_loss: 6.1652e-04 - val_mse: 6.4254e-07\n",
      "Epoch 162/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1454e-04 - mse: 6.4384e-07 - val_loss: 6.1486e-04 - val_mse: 6.4318e-07\n",
      "Epoch 163/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1316e-04 - mse: 6.4248e-07 - val_loss: 6.1246e-04 - val_mse: 6.3665e-07\n",
      "Epoch 164/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1423e-04 - mse: 6.4418e-07 - val_loss: 6.1413e-04 - val_mse: 6.4402e-07\n",
      "Epoch 165/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1431e-04 - mse: 6.4478e-07 - val_loss: 6.1241e-04 - val_mse: 6.4016e-07\n",
      "Epoch 166/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1354e-04 - mse: 6.4347e-07 - val_loss: 6.1289e-04 - val_mse: 6.3836e-07\n",
      "Epoch 167/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1223e-04 - mse: 6.4025e-07 - val_loss: 6.1499e-04 - val_mse: 6.4344e-07\n",
      "Epoch 168/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1372e-04 - mse: 6.4337e-07 - val_loss: 6.1340e-04 - val_mse: 6.4040e-07\n",
      "Epoch 169/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1303e-04 - mse: 6.4207e-07 - val_loss: 6.1405e-04 - val_mse: 6.4028e-07\n",
      "Epoch 170/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1300e-04 - mse: 6.4159e-07 - val_loss: 6.1064e-04 - val_mse: 6.3729e-07\n",
      "Epoch 171/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1153e-04 - mse: 6.3879e-07 - val_loss: 6.1211e-04 - val_mse: 6.3910e-07\n",
      "Epoch 172/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1260e-04 - mse: 6.4209e-07 - val_loss: 6.1379e-04 - val_mse: 6.3780e-07\n",
      "Epoch 173/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1335e-04 - mse: 6.4279e-07 - val_loss: 6.1381e-04 - val_mse: 6.3642e-07\n",
      "Epoch 174/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1428e-04 - mse: 6.4495e-07 - val_loss: 6.1603e-04 - val_mse: 6.4006e-07\n",
      "Epoch 175/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1438e-04 - mse: 6.4329e-07 - val_loss: 6.1296e-04 - val_mse: 6.3999e-07\n",
      "Epoch 176/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1453e-04 - mse: 6.4486e-07 - val_loss: 6.1608e-04 - val_mse: 6.4746e-07\n",
      "Epoch 177/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1421e-04 - mse: 6.4473e-07 - val_loss: 6.1264e-04 - val_mse: 6.4097e-07\n",
      "Epoch 178/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1369e-04 - mse: 6.4297e-07 - val_loss: 6.1392e-04 - val_mse: 6.4256e-07\n",
      "Epoch 179/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1506e-04 - mse: 6.4593e-07 - val_loss: 6.1783e-04 - val_mse: 6.4889e-07\n",
      "Epoch 180/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1513e-04 - mse: 6.4629e-07 - val_loss: 6.1715e-04 - val_mse: 6.4859e-07\n",
      "Epoch 181/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1449e-04 - mse: 6.4477e-07 - val_loss: 6.1447e-04 - val_mse: 6.4138e-07\n",
      "Epoch 182/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1370e-04 - mse: 6.4321e-07 - val_loss: 6.1365e-04 - val_mse: 6.3981e-07\n",
      "Epoch 183/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1373e-04 - mse: 6.4280e-07 - val_loss: 6.1500e-04 - val_mse: 6.4646e-07\n",
      "Epoch 184/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1454e-04 - mse: 6.4446e-07 - val_loss: 6.1354e-04 - val_mse: 6.4076e-07\n",
      "Epoch 185/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1346e-04 - mse: 6.4379e-07 - val_loss: 6.1666e-04 - val_mse: 6.4720e-07\n",
      "Epoch 186/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1421e-04 - mse: 6.4439e-07 - val_loss: 6.1746e-04 - val_mse: 6.4561e-07\n",
      "Epoch 187/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1443e-04 - mse: 6.4508e-07 - val_loss: 6.1285e-04 - val_mse: 6.3672e-07\n",
      "Epoch 188/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1277e-04 - mse: 6.4087e-07 - val_loss: 6.1408e-04 - val_mse: 6.4194e-07\n",
      "Epoch 189/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1362e-04 - mse: 6.4281e-07 - val_loss: 6.1387e-04 - val_mse: 6.4409e-07\n",
      "Epoch 190/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1403e-04 - mse: 6.4341e-07 - val_loss: 6.1924e-04 - val_mse: 6.5270e-07\n",
      "Epoch 191/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1415e-04 - mse: 6.4418e-07 - val_loss: 6.1367e-04 - val_mse: 6.4311e-07\n",
      "Epoch 192/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1305e-04 - mse: 6.4152e-07 - val_loss: 6.1628e-04 - val_mse: 6.4623e-07\n",
      "Epoch 193/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1425e-04 - mse: 6.4414e-07 - val_loss: 6.1624e-04 - val_mse: 6.4359e-07\n",
      "Epoch 194/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1484e-04 - mse: 6.4532e-07 - val_loss: 6.1978e-04 - val_mse: 6.4955e-07\n",
      "Epoch 195/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1453e-04 - mse: 6.4450e-07 - val_loss: 6.1627e-04 - val_mse: 6.4810e-07\n",
      "Epoch 196/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1445e-04 - mse: 6.4491e-07 - val_loss: 6.1530e-04 - val_mse: 6.4150e-07\n",
      "Epoch 197/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1400e-04 - mse: 6.4427e-07 - val_loss: 6.1717e-04 - val_mse: 6.4850e-07\n",
      "Epoch 198/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1584e-04 - mse: 6.4736e-07 - val_loss: 6.1790e-04 - val_mse: 6.4818e-07\n",
      "Epoch 199/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1559e-04 - mse: 6.4706e-07 - val_loss: 6.1375e-04 - val_mse: 6.4310e-07\n",
      "Epoch 200/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1532e-04 - mse: 6.4628e-07 - val_loss: 6.1560e-04 - val_mse: 6.4397e-07\n",
      "Epoch 201/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1481e-04 - mse: 6.4505e-07 - val_loss: 6.1287e-04 - val_mse: 6.3856e-07\n",
      "Epoch 202/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1461e-04 - mse: 6.4618e-07 - val_loss: 6.1656e-04 - val_mse: 6.4085e-07\n",
      "Epoch 203/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1382e-04 - mse: 6.4308e-07 - val_loss: 6.1284e-04 - val_mse: 6.3764e-07\n",
      "Epoch 204/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1220e-04 - mse: 6.3909e-07 - val_loss: 6.1254e-04 - val_mse: 6.4302e-07\n",
      "Epoch 205/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1247e-04 - mse: 6.4118e-07 - val_loss: 6.1479e-04 - val_mse: 6.4357e-07\n",
      "Epoch 206/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1467e-04 - mse: 6.4560e-07 - val_loss: 6.1680e-04 - val_mse: 6.5002e-07\n",
      "Epoch 207/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1571e-04 - mse: 6.4679e-07 - val_loss: 6.1668e-04 - val_mse: 6.5017e-07\n",
      "Epoch 208/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1568e-04 - mse: 6.4698e-07 - val_loss: 6.1650e-04 - val_mse: 6.4899e-07\n",
      "Epoch 209/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1543e-04 - mse: 6.4659e-07 - val_loss: 6.1603e-04 - val_mse: 6.4591e-07\n",
      "Epoch 210/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1401e-04 - mse: 6.4309e-07 - val_loss: 6.1740e-04 - val_mse: 6.4981e-07\n",
      "Epoch 211/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1510e-04 - mse: 6.4708e-07 - val_loss: 6.1486e-04 - val_mse: 6.4344e-07\n",
      "Epoch 212/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1389e-04 - mse: 6.4362e-07 - val_loss: 6.1490e-04 - val_mse: 6.4729e-07\n",
      "Epoch 213/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1391e-04 - mse: 6.4307e-07 - val_loss: 6.1256e-04 - val_mse: 6.3943e-07\n",
      "Epoch 214/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1297e-04 - mse: 6.4263e-07 - val_loss: 6.1497e-04 - val_mse: 6.4294e-07\n",
      "Epoch 215/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1280e-04 - mse: 6.4194e-07 - val_loss: 6.1561e-04 - val_mse: 6.4270e-07\n",
      "Epoch 216/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1393e-04 - mse: 6.4404e-07 - val_loss: 6.1349e-04 - val_mse: 6.3853e-07\n",
      "Epoch 217/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1424e-04 - mse: 6.4462e-07 - val_loss: 6.1415e-04 - val_mse: 6.4119e-07\n",
      "Epoch 218/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1401e-04 - mse: 6.4284e-07 - val_loss: 6.1193e-04 - val_mse: 6.3641e-07\n",
      "Epoch 219/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1329e-04 - mse: 6.4215e-07 - val_loss: 6.1260e-04 - val_mse: 6.4162e-07\n",
      "Epoch 220/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1463e-04 - mse: 6.4525e-07 - val_loss: 6.1392e-04 - val_mse: 6.4248e-07\n",
      "Epoch 221/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1525e-04 - mse: 6.4628e-07 - val_loss: 6.1385e-04 - val_mse: 6.4075e-07\n",
      "Epoch 222/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1483e-04 - mse: 6.4594e-07 - val_loss: 6.1419e-04 - val_mse: 6.3939e-07\n",
      "Epoch 223/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1428e-04 - mse: 6.4471e-07 - val_loss: 6.1681e-04 - val_mse: 6.4615e-07\n",
      "Epoch 224/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1374e-04 - mse: 6.4278e-07 - val_loss: 6.1429e-04 - val_mse: 6.4447e-07\n",
      "Epoch 225/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1359e-04 - mse: 6.4323e-07 - val_loss: 6.1557e-04 - val_mse: 6.4220e-07\n",
      "Epoch 226/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1427e-04 - mse: 6.4353e-07 - val_loss: 6.1343e-04 - val_mse: 6.3957e-07\n",
      "Epoch 227/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1437e-04 - mse: 6.4430e-07 - val_loss: 6.1547e-04 - val_mse: 6.4766e-07\n",
      "Epoch 228/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1442e-04 - mse: 6.4409e-07 - val_loss: 6.1561e-04 - val_mse: 6.4403e-07\n",
      "Epoch 229/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1509e-04 - mse: 6.4571e-07 - val_loss: 6.1507e-04 - val_mse: 6.4444e-07\n",
      "Epoch 230/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1599e-04 - mse: 6.4758e-07 - val_loss: 6.1578e-04 - val_mse: 6.4564e-07\n",
      "Epoch 231/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1589e-04 - mse: 6.4736e-07 - val_loss: 6.1416e-04 - val_mse: 6.4196e-07\n",
      "Epoch 232/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1584e-04 - mse: 6.4744e-07 - val_loss: 6.1289e-04 - val_mse: 6.3792e-07\n",
      "Epoch 233/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1484e-04 - mse: 6.4485e-07 - val_loss: 6.1242e-04 - val_mse: 6.4127e-07\n",
      "Epoch 234/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1428e-04 - mse: 6.4387e-07 - val_loss: 6.1191e-04 - val_mse: 6.3961e-07\n",
      "Epoch 235/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1427e-04 - mse: 6.4499e-07 - val_loss: 6.1570e-04 - val_mse: 6.4400e-07\n",
      "Epoch 236/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1400e-04 - mse: 6.4404e-07 - val_loss: 6.1542e-04 - val_mse: 6.4236e-07\n",
      "Epoch 237/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1423e-04 - mse: 6.4396e-07 - val_loss: 6.1588e-04 - val_mse: 6.4791e-07\n",
      "Epoch 238/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1448e-04 - mse: 6.4422e-07 - val_loss: 6.1421e-04 - val_mse: 6.4286e-07\n",
      "Epoch 239/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1534e-04 - mse: 6.4648e-07 - val_loss: 6.1407e-04 - val_mse: 6.4128e-07\n",
      "Epoch 240/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1475e-04 - mse: 6.4543e-07 - val_loss: 6.1635e-04 - val_mse: 6.4642e-07\n",
      "Epoch 241/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1579e-04 - mse: 6.4844e-07 - val_loss: 6.1662e-04 - val_mse: 6.4513e-07\n",
      "Epoch 242/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1466e-04 - mse: 6.4565e-07 - val_loss: 6.1907e-04 - val_mse: 6.5183e-07\n",
      "Epoch 243/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1547e-04 - mse: 6.4580e-07 - val_loss: 6.1576e-04 - val_mse: 6.4525e-07\n",
      "Epoch 244/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1436e-04 - mse: 6.4463e-07 - val_loss: 6.1649e-04 - val_mse: 6.4388e-07\n",
      "Epoch 245/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1334e-04 - mse: 6.4266e-07 - val_loss: 6.1470e-04 - val_mse: 6.4208e-07\n",
      "Epoch 246/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1353e-04 - mse: 6.4378e-07 - val_loss: 6.1522e-04 - val_mse: 6.4236e-07\n",
      "Epoch 247/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1363e-04 - mse: 6.4259e-07 - val_loss: 6.1369e-04 - val_mse: 6.4241e-07\n",
      "Epoch 248/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1458e-04 - mse: 6.4489e-07 - val_loss: 6.1605e-04 - val_mse: 6.4744e-07\n",
      "Epoch 249/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1623e-04 - mse: 6.4847e-07 - val_loss: 6.1521e-04 - val_mse: 6.4717e-07\n",
      "Epoch 250/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1621e-04 - mse: 6.4774e-07 - val_loss: 6.1426e-04 - val_mse: 6.4433e-07\n",
      "Epoch 251/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1443e-04 - mse: 6.4427e-07 - val_loss: 6.1340e-04 - val_mse: 6.4169e-07\n",
      "Epoch 252/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1247e-04 - mse: 6.4055e-07 - val_loss: 6.1276e-04 - val_mse: 6.4102e-07\n",
      "Epoch 253/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1155e-04 - mse: 6.3980e-07 - val_loss: 6.1449e-04 - val_mse: 6.4504e-07\n",
      "Epoch 254/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1289e-04 - mse: 6.4273e-07 - val_loss: 6.1322e-04 - val_mse: 6.3933e-07\n",
      "Epoch 255/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1321e-04 - mse: 6.4220e-07 - val_loss: 6.1188e-04 - val_mse: 6.3679e-07\n",
      "Epoch 256/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1297e-04 - mse: 6.4127e-07 - val_loss: 6.1300e-04 - val_mse: 6.4376e-07\n",
      "Epoch 257/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1284e-04 - mse: 6.4131e-07 - val_loss: 6.1632e-04 - val_mse: 6.4686e-07\n",
      "Epoch 258/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1460e-04 - mse: 6.4553e-07 - val_loss: 6.1613e-04 - val_mse: 6.4361e-07\n",
      "Epoch 259/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1532e-04 - mse: 6.4645e-07 - val_loss: 6.1449e-04 - val_mse: 6.3846e-07\n",
      "Epoch 260/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1375e-04 - mse: 6.4336e-07 - val_loss: 6.1529e-04 - val_mse: 6.3980e-07\n",
      "Epoch 261/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1371e-04 - mse: 6.4280e-07 - val_loss: 6.1694e-04 - val_mse: 6.4686e-07\n",
      "Epoch 262/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1391e-04 - mse: 6.4375e-07 - val_loss: 6.1427e-04 - val_mse: 6.3905e-07\n",
      "Epoch 263/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1339e-04 - mse: 6.4292e-07 - val_loss: 6.1375e-04 - val_mse: 6.3751e-07\n",
      "Epoch 264/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1275e-04 - mse: 6.4145e-07 - val_loss: 6.1236e-04 - val_mse: 6.3398e-07\n",
      "Epoch 265/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1301e-04 - mse: 6.4217e-07 - val_loss: 6.1231e-04 - val_mse: 6.3474e-07\n",
      "Epoch 266/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1322e-04 - mse: 6.4165e-07 - val_loss: 6.1207e-04 - val_mse: 6.3567e-07\n",
      "Epoch 267/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1427e-04 - mse: 6.4428e-07 - val_loss: 6.1426e-04 - val_mse: 6.4046e-07\n",
      "Epoch 268/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1500e-04 - mse: 6.4532e-07 - val_loss: 6.1382e-04 - val_mse: 6.4346e-07\n",
      "Epoch 269/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1466e-04 - mse: 6.4604e-07 - val_loss: 6.1145e-04 - val_mse: 6.3594e-07\n",
      "Epoch 270/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1294e-04 - mse: 6.4145e-07 - val_loss: 6.1588e-04 - val_mse: 6.4856e-07\n",
      "Epoch 271/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1419e-04 - mse: 6.4405e-07 - val_loss: 6.1694e-04 - val_mse: 6.4889e-07\n",
      "Epoch 272/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1355e-04 - mse: 6.4299e-07 - val_loss: 6.1501e-04 - val_mse: 6.4593e-07\n",
      "Epoch 273/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1212e-04 - mse: 6.4010e-07 - val_loss: 6.1261e-04 - val_mse: 6.3910e-07\n",
      "Epoch 274/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1239e-04 - mse: 6.4040e-07 - val_loss: 6.1336e-04 - val_mse: 6.4294e-07\n",
      "Epoch 275/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1263e-04 - mse: 6.4210e-07 - val_loss: 6.1347e-04 - val_mse: 6.4354e-07\n",
      "Epoch 276/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1399e-04 - mse: 6.4367e-07 - val_loss: 6.1776e-04 - val_mse: 6.4969e-07\n",
      "Epoch 277/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1426e-04 - mse: 6.4358e-07 - val_loss: 6.1601e-04 - val_mse: 6.4631e-07\n",
      "Epoch 278/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1517e-04 - mse: 6.4619e-07 - val_loss: 6.1225e-04 - val_mse: 6.3732e-07\n",
      "Epoch 279/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1504e-04 - mse: 6.4543e-07 - val_loss: 6.1237e-04 - val_mse: 6.3875e-07\n",
      "Epoch 280/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1437e-04 - mse: 6.4413e-07 - val_loss: 6.1239e-04 - val_mse: 6.4115e-07\n",
      "Epoch 281/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1310e-04 - mse: 6.4313e-07 - val_loss: 6.1222e-04 - val_mse: 6.3863e-07\n",
      "Epoch 282/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1288e-04 - mse: 6.4228e-07 - val_loss: 6.1197e-04 - val_mse: 6.3868e-07\n",
      "Epoch 283/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1327e-04 - mse: 6.4302e-07 - val_loss: 6.1686e-04 - val_mse: 6.4908e-07\n",
      "Epoch 284/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1436e-04 - mse: 6.4505e-07 - val_loss: 6.1463e-04 - val_mse: 6.4117e-07\n",
      "Epoch 285/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1392e-04 - mse: 6.4383e-07 - val_loss: 6.1595e-04 - val_mse: 6.4763e-07\n",
      "Epoch 286/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1368e-04 - mse: 6.4335e-07 - val_loss: 6.1353e-04 - val_mse: 6.4136e-07\n",
      "Epoch 287/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1421e-04 - mse: 6.4377e-07 - val_loss: 6.1007e-04 - val_mse: 6.3299e-07\n",
      "Epoch 288/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1359e-04 - mse: 6.4391e-07 - val_loss: 6.1712e-04 - val_mse: 6.4273e-07\n",
      "Epoch 289/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1458e-04 - mse: 6.4423e-07 - val_loss: 6.1704e-04 - val_mse: 6.4950e-07\n",
      "Epoch 290/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1555e-04 - mse: 6.4762e-07 - val_loss: 6.1852e-04 - val_mse: 6.4823e-07\n",
      "Epoch 291/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1402e-04 - mse: 6.4418e-07 - val_loss: 6.1553e-04 - val_mse: 6.4354e-07\n",
      "Epoch 292/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1295e-04 - mse: 6.4186e-07 - val_loss: 6.1480e-04 - val_mse: 6.4440e-07\n",
      "Epoch 293/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1389e-04 - mse: 6.4279e-07 - val_loss: 6.1332e-04 - val_mse: 6.4058e-07\n",
      "Epoch 294/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1316e-04 - mse: 6.4189e-07 - val_loss: 6.1304e-04 - val_mse: 6.4438e-07\n",
      "Epoch 295/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1314e-04 - mse: 6.4196e-07 - val_loss: 6.1550e-04 - val_mse: 6.4767e-07\n",
      "Epoch 296/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1274e-04 - mse: 6.4155e-07 - val_loss: 6.1579e-04 - val_mse: 6.4392e-07\n",
      "Epoch 297/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1321e-04 - mse: 6.4220e-07 - val_loss: 6.1356e-04 - val_mse: 6.3749e-07\n",
      "Epoch 298/300\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6.1372e-04 - mse: 6.4304e-07 - val_loss: 6.1398e-04 - val_mse: 6.3893e-07\n",
      "Epoch 299/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1468e-04 - mse: 6.4450e-07 - val_loss: 6.1107e-04 - val_mse: 6.3922e-07\n",
      "Epoch 300/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.1367e-04 - mse: 6.4321e-07 - val_loss: 6.1533e-04 - val_mse: 6.4273e-07\n",
      "-33.241005871752364\n",
      "0\n",
      "canal numero  0 Saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9387fd8d7517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mgdx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mx_gd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_gd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgdx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mrealx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "IH1=np.zeros((1000, 2000))\n",
    "OH1=np.zeros((1000, 4620))\n",
    "K0, sigma_2d = Basic_Nonlinear_Distortion_Model(A, B, q, p, G, Vsat, IBO)\n",
    "\n",
    "\"\"\" Generate the matrix of the channel H ∈ ℂ(Mr * Mt) whose inputs are complex random Gaussian variables\n",
    "    Transfer H Matrix into a vector that contains H Real values concatenated with H Imaginary values  \"\"\"\n",
    "for t in range(1000):\n",
    "    N_bits = Mr * np.log2(M)\n",
    "    x_gd = np.zeros((Mt, 1))\n",
    "    gdx1 = np.zeros((Mt, Niter))\n",
    "    d = np.zeros((Mt, 1))\n",
    "    S = np.zeros((10000, 20))\n",
    "    X = np.zeros((10000, 200))\n",
    "    H = (1 / np.sqrt(2 * Mt)) * (np.random.randn(Mr, Mt) + 1j * np.random.randn(Mr, Mt))\n",
    "    HR = H.flatten()\n",
    "    realh = np.real(HR)\n",
    "    imagh = np.imag(HR)\n",
    "    Hr = np.concatenate((realh, imagh), axis=0)\n",
    "    Hr = np.reshape(Hr, (2000, 1)).T\n",
    "    HH = Hr\n",
    "    HH.tolist()\n",
    "     \"\"\"Generate for each H; 10,000 symbols Z and 10,000 precoded symbols X by\n",
    "    Generating a random bit stream equivalent to the transmissed signal then modulating this bit stream (16-QAM type modulation) using the QAMModem () function of the CommPy library\n",
    "    Concatenate Z real part with Z imaginary part into S vector its size is (10000,20)\n",
    "    Compute the gradient gradient in order to have X_gd which, when ampliﬁed and then passed through the channel, can guarantee excellent transmission quality.  \n",
    "    The vector X contains precoded real symbols concatenated with the imaginary ones.\n",
    "    Then update IBO Coefficients where Input Back-Off in a power amplifier, is a measure of how far you must reduce the input power in order to receive the desired output linearity and power.\n",
    "    Determine the distortion vector (d) and the PA Gain complex (K0) using hpa_sspa_modif_rapp() and find_K0_sigma2_d() functions.\n",
    "    Where Vin and Vout are the input / output of Power amplifier PA.\"\"\"\n",
    "    for j in range(10000):\n",
    "        bits = np.random.randint(2, size=int(N_bits))  \n",
    "        QAM16 = QAMModem(16)  \n",
    "        z = QAM16.modulate(bits)\n",
    "        Z = np.reshape(z, (1, 10))\n",
    "        reals = np.real(Z)\n",
    "        imags = np.imag(Z)\n",
    "        s = np.concatenate((reals, imags), axis=1)\n",
    "        S[j] = s\n",
    "        for i in range(Niter):\n",
    "            gdx1 = 2 * np.conj(K0 * np.transpose(H)).dot(K0 * H.dot(x_gd) + H.dot(d)- Z.T)\n",
    "            x_gd = x_gd - mu1 * gdx1\n",
    "            realx = np.real(x_gd)\n",
    "            imagx = np.imag(x_gd)\n",
    "            x_z = np.concatenate((realx, imagx), axis=0).reshape((200))\n",
    "            X[j] = x_z\n",
    "            val_IBO_m1dB = (((1 / np.sqrt(10 ** -0.1)) ** (2 * p) - 1) ** (1 / (2 * p)) * Vsat / (G))\n",
    "            coeff_IBO_m1dB = (val_IBO_m1dB * np.sqrt((1 / np.var(x_gd))) * np.sqrt(10 ** (-IBO / 10)))  \n",
    "            vin2 = coeff_IBO_m1dB * x_gd\n",
    "            vout2 = hpa_sspa_modif_rapp(vin2, Vsat, p, q, G, A, B) \n",
    "            K0, sigma2_d = find_K0_sigma2_d(vin2, vout2)\n",
    "            d = vout2 - K0 * vin2  \n",
    "            y_gd =vout2 / coeff_IBO_m1dB\n",
    "        MSEgd=np.mean(np.abs((H.dot(y_gd)) - Z.T) ** 2) / (np.mean(np.abs(Z) ** 2))\n",
    "    NMSEGD = 10 * np.log10(np.mean(MSEgd))\n",
    "    print('nmsede GD',NMSEGD)\n",
    "    \"Base d'apprentissage\"\n",
    "    X_train = S[:9000, :]\n",
    "    y_train = X[:9000, :]\n",
    "    \"Base de test\"\n",
    "    X_test = S[9000:, :]\n",
    "    y_test = X[9000:, :]\n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=20, kernel_initializer=\"uniform\", activation=\"linear\", input_dim=20))\n",
    "    model.add(Dense(units=200, kernel_initializer=\"uniform\", activation=\"linear\"))\n",
    "    model.compile(optimizer=\"Adamax\", loss=\"mae\", metrics=['mse'])\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=1000,\n",
    "        epochs=300,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    #Calcul d NMSE\n",
    "    y_pred = model.predict(X_train)\n",
    "    y_predt = model.predict(X_test)\n",
    "    NMSE = np.zeros([1000, 1])\n",
    "    recievehh = np.zeros([1000, 10], dtype=complex)\n",
    "    Y_gdr = y_predt[:1000, :100]\n",
    "    Y_gdi = 1j * y_predt[:1000, 100:]\n",
    "    Y_gd = Y_gdr + Y_gdi\n",
    "    Shr = X_test[:1000, :10]\n",
    "    Shi = 1j * X_test[:1000, 10:20]\n",
    "    SSh = Shr + Shi\n",
    "    for i in range(1000):\n",
    "        val_IBO_m1dB = (((1 / np.sqrt(10 ** -0.1)) ** (2 * p) - 1) ** (1 / (2 * p)) * Vsat / (G))\n",
    "        coeff_IBO_m1dB = (val_IBO_m1dB * np.sqrt((1 / np.var(Y_gd[i]))) * np.sqrt(10 ** (-IBO / 10)))\n",
    "        vin22 = coeff_IBO_m1dB * Y_gd[i]\n",
    "        vout22 = hpa_sspa_modif_rapp(vin22, Vsat, p, q, G, A, B)\n",
    "        Y_gd_amp = vout22 / coeff_IBO_m1dB\n",
    "        recieveh = (H.dot(Y_gd_amp)).reshape((10))\n",
    "        recievehh[i] = recieveh\n",
    "        NMSE[i] = np.mean(np.abs(SSh[i] - recieveh) ** 2) / np.mean(np.abs(SSh[i]) ** 2)\n",
    "    NMSEdb = 10 * np.log10(np.mean(NMSE))\n",
    "    print(NMSEdb )\n",
    "    if NMSEdb< -32 :\n",
    "        \"\"\" Recover the weight vector of our model NN1 using the function get_weights ().\n",
    "        This vector will be the desired output of the NN2 model.\"\"\"\n",
    "        first_layer_weights =model.layers[0].get_weights()[0] \n",
    "        \n",
    "        first=first_layer_weights.flatten()\n",
    "        first_layer_biases  = model.layers[0].get_weights()[1]\n",
    "        first_layer=np.concatenate((first,first_layer_biases),axis=0)\n",
    "        second_layer_weights = model.layers[1].get_weights()[0]\n",
    "        second=second_layer_weights.flatten()\n",
    "        second_layer_biases  = model.layers[1].get_weights()[1]\n",
    "        second_layer=np.concatenate((second,second_layer_biases),axis=0)\n",
    "        weights=np.concatenate((first_layer,second_layer),axis=0)\n",
    "        weights.tolist()\n",
    "        print (t)\n",
    "        IH1[t]=HH\n",
    "        OH1[t]=weights\n",
    "        save('dataI28.npy',IH1)\n",
    "        save('dataO28.npy',OH1)\n",
    "        print('canal numero ',t,'Saved')\n",
    "    else:\n",
    "        print('canal numero ',t,'Condition not satisfied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109020, 2000)\n",
      "(109020, 4620)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NN2 Database Preparation \"\"\"\n",
    "IH1=load('dataI1.npy')[np.nonzero(load('dataI1.npy' ))]\n",
    "OH1=load('dataO1.npy')[np.nonzero(load('dataO1.npy' ))]\n",
    "IH2=load('dataI2.npy')[np.nonzero(load('dataI2.npy' ))]\n",
    "OH2=load('dataO2.npy')[np.nonzero(load('dataO2.npy' ))]\n",
    "IH3=load('dataI3.npy')[np.nonzero(load('dataI3.npy' ))]\n",
    "OH3=load('dataO3.npy')[np.nonzero(load('dataO3.npy' ))]\n",
    "IH6=load('dataI6.npy')[np.nonzero(load('dataI6.npy' ))]\n",
    "OH6=load('dataO6.npy')[np.nonzero(load('dataO6.npy' ))]\n",
    "IH5=load('dataI5.npy')[np.nonzero(load('dataI5.npy'))]\n",
    "OH5=load('dataO5.npy')[np.nonzero(load('dataO5.npy'))]\n",
    "IH4=load('dataI4.npy')[np.nonzero(load('dataI4.npy' ))]\n",
    "OH4=load('dataO4.npy')[np.nonzero(load('dataO4.npy' ))]\n",
    "IH8=load('dataI8.npy')[np.nonzero(load('dataI8.npy' ))]\n",
    "OH8=load('dataO8.npy')[np.nonzero(load('dataO8.npy' ))]\n",
    "IH9=load('dataI9.npy')[np.nonzero(load('dataI9.npy'))]\n",
    "OH9=load('dataO9.npy')[np.nonzero(load('dataO9.npy'))]\n",
    "IH10=load('dataI10.npy')[np.nonzero(load('dataI10.npy' ))]\n",
    "OH10=load('dataO10.npy')[np.nonzero(load('dataO10.npy' ))]\n",
    "IH11=load('dataI11.npy')[np.nonzero(load('dataI11.npy' ))]\n",
    "OH11=load('dataO11.npy')[np.nonzero(load('dataO11.npy' ))]\n",
    "IH12=load('dataI12.npy')[np.nonzero(load('dataI12.npy'))]\n",
    "OH12=load('dataO12.npy')[np.nonzero(load('dataO12.npy'))]\n",
    "IH7=load('dataI7.npy')[np.nonzero(load('dataI7.npy' ))]\n",
    "OH7=load('dataO7.npy')[np.nonzero(load('dataO7.npy' ))]\n",
    "IH13=load('dataI13.npy')[np.nonzero(load('dataI13.npy'))]\n",
    "OH13=load('dataO13.npy')[np.nonzero(load('dataO13.npy'))]\n",
    "IH14=load('dataI14.npy')[np.nonzero(load('dataI14.npy'))]\n",
    "OH14=load('dataO14.npy')[np.nonzero(load('dataO14.npy'))]\n",
    "IH15=load('dataI15.npy')[np.nonzero(load('dataI15.npy'))]\n",
    "OH15=load('dataO15.npy')[np.nonzero(load('dataO15.npy'))]\n",
    "IH16=load('dataI16.npy')[np.nonzero(load('dataI16.npy'))]\n",
    "OH16=load('dataO16.npy')[np.nonzero(load('dataO16.npy'))]\n",
    "IH17=load('dataI17.npy')[np.nonzero(load('dataI17.npy'))]\n",
    "OH17=load('dataO17.npy')[np.nonzero(load('dataO17.npy'))]\n",
    "IH18=load('dataI18.npy')[np.nonzero(load('dataI18.npy'))]\n",
    "OH18=load('dataO18.npy')[np.nonzero(load('dataO18.npy'))]\n",
    "IH19=load('dataI19.npy')[np.nonzero(load('dataI19.npy'))]\n",
    "OH19=load('dataO19.npy')[np.nonzero(load('dataO19.npy'))]\n",
    "IH20=load('dataI20.npy')[np.nonzero(load('dataI20.npy'))]\n",
    "OH20=load('dataO20.npy')[np.nonzero(load('dataO20.npy'))]\n",
    "IH21=load('dataI21.npy')[np.nonzero(load('dataI21.npy'))]\n",
    "OH21=load('dataO21.npy')[np.nonzero(load('dataO21.npy'))]\n",
    "IH22=load('dataI22.npy')[np.nonzero(load('dataI22.npy'))]\n",
    "OH22=load('dataO22.npy')[np.nonzero(load('dataO22.npy'))]\n",
    "IH23=load('dataI23.npy')[np.nonzero(load('dataI23.npy'))]\n",
    "OH23=load('dataO23.npy')[np.nonzero(load('dataO23.npy'))]\n",
    "IH24=load('dataI24.npy')[np.nonzero(load('dataI24.npy'))]\n",
    "OH24=load('dataO24.npy')[np.nonzero(load('dataO24.npy'))]\n",
    "IH25=load('dataI25.npy')[np.nonzero(load('dataI25.npy'))]\n",
    "OH25=load('dataO25.npy')[np.nonzero(load('dataO25.npy'))]\n",
    "IH26=load('dataI26.npy')[np.nonzero(load('dataI26.npy'))]\n",
    "OH26=load('dataO26.npy')[np.nonzero(load('dataO26.npy'))]\n",
    "IH27=load('dataI27.npy')[np.nonzero(load('dataI27.npy'))]\n",
    "OH27=load('dataO27.npy')[np.nonzero(load('dataO27.npy'))]\n",
    "IHH=np.concatenate((IH1,IH2,IH3,IH4,IH5,IH6,IH8,IH9,IH10,IH11,IH12,IH7,IH13,IH14,IH15,IH16,IH17,IH18,IH19,IH20,IH21,IH22,IH23,IH24,IH25,IH26,IH27),axis=0)\n",
    "OHH=np.concatenate((OH1,OH2,OH3,OH4,OH5,OH6,OH8,OH9,OH10,OH11,OH12,OH7,OH13,OH14,OH15,OH16,OH17,OH18,OH19,OH20,OH21,OH22,OH23,OH24,OH25,OH26,OH27),axis=0)\n",
    "# creating a noise\n",
    "IHHHH=[]\n",
    "OHHHH=[]\n",
    "for w in range(5):\n",
    "    mu, sigma = 0, 0.0001 \n",
    "    noise = np.random.normal(mu, sigma, [50367240,]) \n",
    "    noiseW=OHH+noise\n",
    "    IHHH=np.concatenate((IHH, np.copy(IHH)),axis=0)\n",
    "    IHHHH=np.append(IHHHH, IHHH)\n",
    "    OHHH=np.concatenate((OHH, noiseW),axis=0)\n",
    "    OHHHH=np.append(OHHHH,OHHH)\n",
    "IHHHH=IHHHH.reshape((109020,2000))\n",
    "OHHHH=OHHHH.reshape((109020,4620))\n",
    "print(IHHHH.shape)    \n",
    "print(OHHHH.shape)\n",
    "X_trainN2 = IHHHH[:100000,:]\n",
    "X_trainN2 = X_trainN2.reshape((X_trainN2.shape[0], X_trainN2.shape[1], 1))\n",
    "y_trainN2 = OHHHH[:100000,:]\n",
    "y_trainN2 = y_trainN2.reshape((y_trainN2.shape[0], y_trainN2.shape[1], 1))\n",
    "X_testN2= IHHHH[100000:,:]\n",
    "X_testN2 = X_testN2.reshape((X_testN2.shape[0], X_testN2.shape[1], 1))\n",
    "y_testN2 = OHHHH[100000:,:]\n",
    "y_testN2 = y_testN2.reshape((y_testN2.shape[0], y_testN2.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1)\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1230 mean_absolute_error\n        return K.mean(math_ops.abs(y_pred - y_true), axis=-1)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:984 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:10103 sub\n        \"Sub\", x=x, y=y, name=name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal\n        compute_device)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal\n        op_def=op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1817 __init__\n        control_input_ops, op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 1994 and 4620 for '{{node mean_absolute_error/sub}} = Sub[T=DT_FLOAT](sequential_78/dense_143/BiasAdd, IteratorGetNext:1)' with input shapes: [1000,1994,4620], [1000,4620,1].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-5200e204f9cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4620\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Adamax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhistory2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainN2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trainN2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_testN2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_testN2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# Loss representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1230 mean_absolute_error\n        return K.mean(math_ops.abs(y_pred - y_true), axis=-1)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:984 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:10103 sub\n        \"Sub\", x=x, y=y, name=name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal\n        compute_device)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal\n        op_def=op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1817 __init__\n        control_input_ops, op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 1994 and 4620 for '{{node mean_absolute_error/sub}} = Sub[T=DT_FLOAT](sequential_78/dense_143/BiasAdd, IteratorGetNext:1)' with input shapes: [1000,1994,4620], [1000,4620,1].\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create NN2 Model which contains 2 hidden layers; \n",
    "The first layer have 1000 neurones (input_dim=2000)\n",
    "The second have 4620 neurones (output_dim=4620) \"\"\"\n",
    "import tensorflow as tf\n",
    "keras.optimizers.Adamax(learning_rate=0.9, beta_1=0.999, beta_2=0.999)\n",
    "in_shape=X_trainN2.shape[1:]\n",
    "print (in_shape)\n",
    "modell=Sequential()\n",
    "modell.add(Conv1D(filters=30, kernel_size=7, strides=1,activation='linear',input_shape=in_shape))\n",
    "\"\"\"model.add(Conv1D(30, 5, activation='relu')) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(30, 4, activation='relu')) \n",
    "model.add(MaxPooling1D(3))\"\"\"\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1000, activation=\"linear\"))\n",
    "modell.add(Dense(units=4620, kernel_initializer=\"uniform\", activation=\"linear\"))\n",
    "modell.compile(optimizer=\"Adamax\", loss=\"mae\", metrics=['mse'])\n",
    "history2 = modell.fit(X_trainN2,y_trainN2,validation_data=(X_testN2, y_testN2),batch_size=1000,epochs=200,shuffle=True,verbose = 2)\n",
    "# Loss representation\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.grid(True,which=\"both\", linestyle='--')\n",
    "plt.ylabel('Loss') \n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Test'], loc='upper left')\n",
    "plt.show()\n",
    "# Make predictions\n",
    "y_pred2 = modell.predict(X_trainN2)\n",
    "y_predt2 = modell.predict(X_testN2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_59 (Conv1D)           (None, 1994, 30)          240       \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 1994, 4620)        143220    \n",
      "=================================================================\n",
      "Total params: 143,460\n",
      "Trainable params: 143,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modell.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred2 = modell.predict(X_trainN2)\n",
    "y_predt2 = modell.predict(X_testN2)\n",
    "\n",
    "# Loss representation\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.grid(True,which=\"both\", linestyle='--')\n",
    "plt.ylabel('Loss') \n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94020, 400)\n",
      "(94020, 20)\n",
      "(94020, 4000)\n",
      "(94020, 200)\n"
     ]
    }
   ],
   "source": [
    "first_layer_weightstt=y_predt2[:,:400]\n",
    "print(first_layer_weightstt.shape)\n",
    "first_layer_biasestt=y_predt2[:,400:420]\n",
    "print(first_layer_biasestt.shape)\n",
    "second_layer_weightstt=y_predt2[:,420:4420]\n",
    "print(second_layer_weightstt.shape)\n",
    "second_layer_biasestt=y_predt2[:,4420:]\n",
    "print(second_layer_biasestt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "Ih11=load('IH11.npy')[np.nonzero(load('IH11.npy' ))]\n",
    "Oh11=load('OH11.npy')[np.nonzero(load('OH11.npy' ))]\n",
    "Ih11=Ih11.reshape((429,2000))\n",
    "Oh11=Oh11.reshape((429,4620))\n",
    "y_predt3= modell.predict(Ih11[:1,:])\n",
    "print(mean_absolute_error(y_predt3[:1,:],Oh11[:1,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y_predt[1,:100],'b')\n",
    "plt.plot(y_testN2[1,:100],'r')\n",
    "ERR=(abs(y_predt2[5,:]-y_testN2[5,:])**2)\n",
    "print(sum(ERR))\n",
    "plt.figure()\n",
    "plt.plot(ERR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canal numero: 0\n",
      "(900, 100)\n",
      "NMSE de canal numero 0 est:  -0.047953969855796495\n",
      "Canal numero: 1\n",
      "(900, 100)\n",
      "NMSE de canal numero 1 est:  -0.061728517950511874\n",
      "Canal numero: 2\n",
      "(900, 100)\n",
      "NMSE de canal numero 2 est:  -0.02399007361386424\n",
      "Canal numero: 3\n",
      "(900, 100)\n",
      "NMSE de canal numero 3 est:  -0.07012059761866901\n",
      "Canal numero: 4\n",
      "(900, 100)\n",
      "NMSE de canal numero 4 est:  -0.020423575483262648\n",
      "Canal numero: 5\n",
      "(900, 100)\n",
      "NMSE de canal numero 5 est:  -0.05826881002975751\n",
      "Canal numero: 6\n",
      "(900, 100)\n",
      "NMSE de canal numero 6 est:  -0.07178765599084459\n",
      "Canal numero: 7\n",
      "(900, 100)\n",
      "NMSE de canal numero 7 est:  -0.03809120469274066\n",
      "Canal numero: 8\n",
      "(900, 100)\n",
      "NMSE de canal numero 8 est:  -0.07144692994304687\n",
      "Canal numero: 9\n",
      "(900, 100)\n",
      "NMSE de canal numero 9 est:  -0.019006906665060858\n",
      "Canal numero: 10\n",
      "(900, 100)\n",
      "NMSE de canal numero 10 est:  0.0020058212758908476\n",
      "Canal numero: 11\n",
      "(900, 100)\n",
      "NMSE de canal numero 11 est:  -0.045238942090756386\n",
      "Canal numero: 12\n",
      "(900, 100)\n",
      "NMSE de canal numero 12 est:  -0.007393417974582285\n",
      "Canal numero: 13\n",
      "(900, 100)\n",
      "NMSE de canal numero 13 est:  -0.04492021596428088\n",
      "Canal numero: 14\n",
      "(900, 100)\n",
      "NMSE de canal numero 14 est:  -0.024047663446482084\n",
      "Canal numero: 15\n",
      "(900, 100)\n",
      "NMSE de canal numero 15 est:  -0.07598471661791042\n",
      "Canal numero: 16\n",
      "(900, 100)\n",
      "NMSE de canal numero 16 est:  -0.05951221287847222\n",
      "Canal numero: 17\n",
      "(900, 100)\n",
      "NMSE de canal numero 17 est:  -0.019135705461223975\n",
      "Canal numero: 18\n",
      "(900, 100)\n",
      "NMSE de canal numero 18 est:  -0.000393090496591197\n",
      "Canal numero: 19\n",
      "(900, 100)\n",
      "NMSE de canal numero 19 est:  -0.05999934493105945\n",
      "Canal numero: 20\n",
      "(900, 100)\n",
      "NMSE de canal numero 20 est:  -0.010632340036688078\n",
      "Canal numero: 21\n",
      "(900, 100)\n",
      "NMSE de canal numero 21 est:  -0.004931405764785833\n",
      "Canal numero: 22\n",
      "(900, 100)\n",
      "NMSE de canal numero 22 est:  -0.018930648792995965\n",
      "Canal numero: 23\n",
      "(900, 100)\n",
      "NMSE de canal numero 23 est:  -0.03992690430277687\n",
      "Canal numero: 24\n",
      "(900, 100)\n",
      "NMSE de canal numero 24 est:  -0.07194710925773047\n",
      "Canal numero: 25\n",
      "(900, 100)\n",
      "NMSE de canal numero 25 est:  -0.047136382758678555\n",
      "Canal numero: 26\n",
      "(900, 100)\n",
      "NMSE de canal numero 26 est:  0.01072993991431026\n",
      "Canal numero: 27\n",
      "(900, 100)\n",
      "NMSE de canal numero 27 est:  -0.035149705756773265\n",
      "Canal numero: 28\n",
      "(900, 100)\n",
      "NMSE de canal numero 28 est:  -0.021531053768360467\n",
      "Canal numero: 29\n",
      "(900, 100)\n",
      "NMSE de canal numero 29 est:  -0.03732103192746079\n",
      "Canal numero: 30\n",
      "(900, 100)\n",
      "NMSE de canal numero 30 est:  -0.001092835598437762\n",
      "Canal numero: 31\n",
      "(900, 100)\n",
      "NMSE de canal numero 31 est:  -0.007539997549874621\n",
      "Canal numero: 32\n",
      "(900, 100)\n",
      "NMSE de canal numero 32 est:  -0.03172051142478161\n",
      "Canal numero: 33\n",
      "(900, 100)\n",
      "NMSE de canal numero 33 est:  -0.04432019027621279\n",
      "Canal numero: 34\n",
      "(900, 100)\n",
      "NMSE de canal numero 34 est:  -0.032670118243940637\n",
      "Canal numero: 35\n",
      "(900, 100)\n",
      "NMSE de canal numero 35 est:  -0.06388439892107926\n",
      "Canal numero: 36\n",
      "(900, 100)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-79c7120849d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mval_IBO_m1dB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mVsat\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mcoeff_IBO_m1dB22\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_IBO_m1dB\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_gd2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mIBO\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mvin222\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeff_IBO_m1dB22\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY_gd2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mvout222\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhpa_sspa_modif_rapp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvin222\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVsat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3367\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0marrmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplexfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "for t in range(900):\n",
    "    NMSE2 = np.zeros((900, 1))\n",
    "    H2r=X_testN2[t,:1000]\n",
    "    H2i=1j*X_testN2[t,1000:]\n",
    "    H02=H2r+H2i\n",
    "    H2=H02.reshape((10,100))\n",
    "    print('Canal numero:',t)\n",
    "    S_NN1 =X_test[:900,:]\n",
    "    modelll=Sequential()\n",
    "    modelll.add(Dense(units=20, kernel_initializer=\"uniform\", activation=\"linear\", input_dim=20))\n",
    "    first_weightstt=first_layer_weightstt[t,:400]\n",
    "    L1=first_weightstt.reshape((20,20))\n",
    "    first_layer_biasestt=y_predt2[t,400:420]\n",
    "    L2=first_layer_biasestt\n",
    "    modelll.layers[0].set_weights([L1,L2])\n",
    "    modelll.add(Dense(units=200, kernel_initializer=\"uniform\", activation=\"linear\"))\n",
    "    second_weightstt=y_predt2[t,420:4420]\n",
    "    L3=second_weightstt.reshape((20,200))\n",
    "    second_layer_biasestt=y_predt2[t,4420:4620]\n",
    "    L4=second_layer_biasestt\n",
    "    modelll.layers[1].set_weights([L3,L4]) \n",
    "    y_predt22 = modelll.predict(S_NN1)\n",
    "    Y_gd2r = y_predt22[:900,:100]\n",
    "    Y_gd2i = 1j * y_predt22[:900,100:]\n",
    "    Y_gd2 = Y_gd2r + Y_gd2i\n",
    "    print(Y_gd2.shape)\n",
    "    Shr2 = S_NN1[:900, :10]\n",
    "    Shi2 = 1j * S_NN1[:900, 10:20]\n",
    "    SSh2 = Shr2 + Shi2\n",
    "    for n in range(900):\n",
    "        val_IBO_m1dB = (((1 / np.sqrt(10 ** -0.1)) ** (2 * p) - 1) ** (1 / (2 * p)) * Vsat / (G))\n",
    "        coeff_IBO_m1dB22=val_IBO_m1dB * np.sqrt((1 / np.var(Y_gd2[n]))) * np.sqrt(10 ** (-IBO / 10))\n",
    "        vin222 = coeff_IBO_m1dB22 * Y_gd2[n]\n",
    "        vout222 = hpa_sspa_modif_rapp(vin222, Vsat, p, q, G, A, B)\n",
    "        Y_gd_amp2 = vout222 / coeff_IBO_m1dB22\n",
    "        recieveh2 = (H2.dot(Y_gd_amp2)).reshape((10))\n",
    "        NMSE2[n]= np.mean(np.abs(SSh2[n] - recieveh2) ** 2) / np.mean(np.abs(SSh2[n]) ** 2)\n",
    "    NMSE2db = 10 * np.log10(np.mean(NMSE2)) \n",
    "    print('NMSE de canal numero',t ,'est: ',NMSE2db) \n",
    "    #print('R',recieveh2)\n",
    "    #print( 'SSH',SSh2)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canal numero: 0\n",
      "(900, 100)\n",
      "NMSE de canal numero 0 est:  -32.760072986034665\n",
      "Canal numero: 1\n",
      "(900, 100)\n",
      "NMSE de canal numero 1 est:  -33.191773759648086\n",
      "Canal numero: 2\n",
      "(900, 100)\n",
      "NMSE de canal numero 2 est:  -32.80616906628368\n",
      "Canal numero: 3\n",
      "(900, 100)\n",
      "NMSE de canal numero 3 est:  -32.8390884646831\n",
      "Canal numero: 4\n",
      "(900, 100)\n",
      "NMSE de canal numero 4 est:  -32.699499254037804\n",
      "Canal numero: 5\n",
      "(900, 100)\n",
      "NMSE de canal numero 5 est:  -32.747793400864836\n",
      "Canal numero: 6\n",
      "(900, 100)\n",
      "NMSE de canal numero 6 est:  -32.753637132192324\n",
      "Canal numero: 7\n",
      "(900, 100)\n",
      "NMSE de canal numero 7 est:  -33.02459679291748\n",
      "Canal numero: 8\n",
      "(900, 100)\n",
      "NMSE de canal numero 8 est:  -32.77775979846243\n",
      "Canal numero: 9\n",
      "(900, 100)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f0d3fffa6f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mcoeff_IBO_m1dB22\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_IBO_m1dB\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_gd2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mIBO\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mvin222\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeff_IBO_m1dB22\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY_gd2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mvout222\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhpa_sspa_modif_rapp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvin222\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVsat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mY_gd_amp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvout222\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcoeff_IBO_m1dB22\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mrecieveh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_gd_amp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/Utils.ipynb\u001b[0m in \u001b[0;36mhpa_sspa_modif_rapp\u001b[0;34m(vin, Vsat, p, q, G, A, B)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "for t in range(900):\n",
    "    H2r=X_testN2[t,:1000]\n",
    "    H2i=1j*X_testN2[t,1000:]\n",
    "    H02=H2r+H2i\n",
    "    H2=H02.reshape((10,100))\n",
    "    print('Canal numero:',t)\n",
    "    S_NN1 =X_test[:900,:]\n",
    "    modelll=Sequential()\n",
    "    modelll.add(Dense(units=20, kernel_initializer=\"uniform\", activation=\"linear\", input_dim=20))\n",
    "    first_weightsttV=y_testN2[t,:400]\n",
    "    LV1=first_weightsttV.reshape((20,20))\n",
    "    first_layer_biasesttV=y_testN2[t,400:420]\n",
    "    LV2=first_layer_biasesttV\n",
    "    modelll.layers[0].set_weights([LV1,LV2])\n",
    "    modelll.add(Dense(units=200, kernel_initializer=\"uniform\", activation=\"linear\"))\n",
    "    second_weightsttV=y_testN2[t,420:4420]\n",
    "    LV3=second_weightsttV.reshape((20,200))\n",
    "    second_layer_biasesttV=y_testN2[t,4420:4620]\n",
    "    LV4=second_layer_biasesttV\n",
    "    modelll.layers[1].set_weights([LV3,LV4]) \n",
    "    y_predt22v = modelll.predict(S_NN1)\n",
    "    NMSE22 = np.zeros((900, 1))\n",
    "    Y_gd2r = y_predt22v[:900,:100]\n",
    "    Y_gd2i = 1j * y_predt22v[:900,100:]\n",
    "    Y_gd2 = Y_gd2r + Y_gd2i\n",
    "    print(Y_gd2.shape)\n",
    "    Shr2 = S_NN1[:900, :10]\n",
    "    Shi2 = 1j * S_NN1[:900, 10:20]\n",
    "    SSh2 = Shr2 + Shi2\n",
    "    for n in range(900):\n",
    "        val_IBO_m1dB = (((1 / np.sqrt(10 ** -0.1)) ** (2 * p) - 1) ** (1 / (2 * p)) * Vsat / (G))\n",
    "        coeff_IBO_m1dB22=val_IBO_m1dB * np.sqrt((1 / np.var(Y_gd2[n]))) * np.sqrt(10 ** (-IBO / 10))\n",
    "        vin222 = coeff_IBO_m1dB22 * Y_gd2[n]\n",
    "        vout222 = hpa_sspa_modif_rapp(vin222, Vsat, p, q, G, A, B)\n",
    "        Y_gd_amp2 = vout222 / coeff_IBO_m1dB22\n",
    "        recieveh2 = (H2.dot(Y_gd_amp2)).reshape((10))\n",
    "        NMSE22[n]= np.mean(np.abs(SSh2[n] - recieveh2) ** 2) / np.mean(np.abs(SSh2[n]) ** 2)\n",
    "    NMSE22db = 10 * np.log10(np.mean(NMSE22)) \n",
    "    print('NMSE de canal numero',t ,'est: ',NMSE22db) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
